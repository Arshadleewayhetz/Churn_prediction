{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Churn Prediction Statistical Analysis\n",
    "\n",
    "This notebook implements a comprehensive statistical model to identify churn reasons and their impact percentages. It handles highly skewed data with more than 80% zero values in many features.\n",
    "\n",
    "## Approach\n",
    "\n",
    "1. **Data Preprocessing**:\n",
    "   - Handle missing values and zero values\n",
    "   - Remove features with >80% missing or zero values\n",
    "   - Remove data leakage columns\n",
    "   - Standardize numerical features\n",
    "   - Encode categorical features\n",
    "\n",
    "2. **Statistical Modeling**:\n",
    "   - Train multiple classification models (Logistic Regression, Random Forest, Gradient Boosting)\n",
    "   - Perform survival analysis using Cox Proportional Hazards model\n",
    "   - Analyze feature importance using multiple methods\n",
    "\n",
    "3. **Churn Reason Identification**:\n",
    "   - Identify top features contributing to churn\n",
    "   - Calculate percentage impact of each feature\n",
    "   - Provide actionable insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, precision_recall_curve\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.inspection import permutation_importance\n",
    "import shap\n",
    "from lifelines import CoxPHFitter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load the dataset\n",
    "try:\n",
    "    df = pd.read_csv('../Churn/features_extracted.csv')\n",
    "    print(f\"Dataset loaded successfully with shape: {df.shape}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading dataset: {e}\")\n",
    "    # Try alternative path\n",
    "    try:\n",
    "        df = pd.read_csv('../Churn/features_extracted (1).csv')\n",
    "        print(f\"Dataset loaded successfully with shape: {df.shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading alternative dataset: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Display basic information about the dataset\n",
    "print(\"Dataset information:\")\n",
    "print(f\"Number of rows: {df.shape[0]}\")\n",
    "print(f\"Number of columns: {df.shape[1]}\")\n",
    "print(\"\\nTarget variable distribution:\")\n",
    "print(df['lost_program'].value_counts(normalize=True) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Replace infinity values with NaNs\n",
    "df = df.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "# Check missing values\n",
    "missing_values = df.isnull().mean() * 100\n",
    "print(\"\\nPercentage of missing values per column (top 20):\")\n",
    "print(missing_values[missing_values > 0].sort_values(ascending=False).head(20))\n",
    "\n",
    "# Check zero values\n",
    "zero_values = (df == 0).mean() * 100\n",
    "print(\"\\nPercentage of zero values per column (>80%):\")\n",
    "print(zero_values[zero_values > 80].sort_values(ascending=False).head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def preprocess_data(df, missing_threshold=80, zero_threshold=80):\n",
    "    \"\"\"Preprocess the data by handling missing values, removing leakage, etc.\"\"\"\n",
    "    print(\"\\nPreprocessing data...\")\n",
    "    \n",
    "    # Remove data leakage columns\n",
    "    leakage_columns = ['churn_date', 'days_since_last_activity']\n",
    "    id_columns = ['datalakeProgramId']\n",
    "    date_columns = [col for col in df.columns if 'date' in col.lower()]\n",
    "    \n",
    "    # Keep some date columns that might be useful for time-based analysis\n",
    "    keep_date_cols = ['startDate', 'endDate', 'last_activity_date']\n",
    "    date_columns = [col for col in date_columns if col not in keep_date_cols]\n",
    "    \n",
    "    drop_columns = leakage_columns + id_columns + date_columns\n",
    "    df = df.drop(columns=[col for col in drop_columns if col in df.columns])\n",
    "    \n",
    "    # Convert categorical columns\n",
    "    categorical_columns = ['siteCenter', 'customerSize']\n",
    "    for col in categorical_columns:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].astype('category')\n",
    "    \n",
    "    # Remove columns with high missing values\n",
    "    missing_values = df.isnull().mean() * 100\n",
    "    high_missing_cols = missing_values[missing_values > missing_threshold].index.tolist()\n",
    "    df = df.drop(columns=high_missing_cols)\n",
    "    \n",
    "    # Remove columns with high zero values\n",
    "    zero_values = (df == 0).mean() * 100\n",
    "    high_zero_cols = zero_values[zero_values > zero_threshold].index.tolist()\n",
    "    df = df.drop(columns=high_zero_cols)\n",
    "    \n",
    "    print(f\"Removed {len(high_missing_cols)} columns with >{missing_threshold}% missing values\")\n",
    "    print(f\"Removed {len(high_zero_cols)} columns with >{zero_threshold}% zero values\")\n",
    "    print(f\"Final dataset shape: {df.shape}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Preprocess the data\n",
    "df_processed = preprocess_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Split the data into training and testing sets\n",
    "X = df_processed.drop(columns=['lost_program'])\n",
    "y = df_processed['lost_program']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training set shape: {X_train.shape}\")\n",
    "print(f\"Testing set shape: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering and Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Identify numerical and categorical columns\n",
    "numerical_cols = X_train.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "categorical_cols = X_train.select_dtypes(include=['category']).columns.tolist()\n",
    "\n",
    "print(f\"Number of numerical features: {len(numerical_cols)}\")\n",
    "print(f\"Number of categorical features: {len(categorical_cols)}\")\n",
    "\n",
    "# Create preprocessing pipelines\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "])\n",
    "\n",
    "# Combine preprocessing steps\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_cols),\n",
    "        ('cat', categorical_transformer, categorical_cols)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Define models to train\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000, class_weight='balanced', random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, class_weight='balanced', random_state=42),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
    "}\n",
    "\n",
    "# Train and evaluate each model\n",
    "results = {}\n",
    "best_model = None\n",
    "best_score = 0\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "    \n",
    "    # Create pipeline with preprocessing\n",
    "    pipeline = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('model', model)\n",
    "    ])\n",
    "    \n",
    "    # Train the model\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    y_prob = pipeline.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Evaluate the model\n",
    "    accuracy = pipeline.score(X_test, y_test)\n",
    "    auc = roc_auc_score(y_test, y_prob)\n",
    "    \n",
    "    print(f\"{name} - Accuracy: {accuracy:.4f}, AUC: {auc:.4f}\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    # Store results\n",
    "    results[name] = {\n",
    "        'pipeline': pipeline,\n",
    "        'accuracy': accuracy,\n",
    "        'auc': auc,\n",
    "        'y_pred': y_pred,\n",
    "        'y_prob': y_prob\n",
    "    }\n",
    "    \n",
    "    # Track best model\n",
    "    if auc > best_score:\n",
    "        best_score = auc\n",
    "        best_model = name\n",
    "\n",
    "print(f\"\\nBest model: {best_model} with AUC: {best_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Get the best model\n",
    "pipeline = results[best_model]['pipeline']\n",
    "model = pipeline.named_steps['model']\n",
    "\n",
    "# Get preprocessed data\n",
    "X_train_processed = pipeline.named_steps['preprocessor'].transform(X_train)\n",
    "\n",
    "# Get feature names after preprocessing\n",
    "feature_names = []\n",
    "\n",
    "# Add numerical feature names\n",
    "feature_names.extend(numerical_cols)\n",
    "\n",
    "# Add one-hot encoded feature names\n",
    "if categorical_cols:\n",
    "    ohe = pipeline.named_steps['preprocessor'].transformers_[1][1].named_steps['onehot']\n",
    "    cat_feature_names = ohe.get_feature_names_out(categorical_cols)\n",
    "    feature_names.extend(cat_feature_names)\n",
    "\n",
    "# Get feature importance based on model type\n",
    "if best_model == 'Logistic Regression':\n",
    "    # For logistic regression, use coefficients\n",
    "    importance = np.abs(model.coef_[0])\n",
    "    \n",
    "elif best_model == 'Random Forest' or best_model == 'Gradient Boosting':\n",
    "    # For tree-based models, use feature importance\n",
    "    importance = model.feature_importances_\n",
    "    \n",
    "    # Also calculate permutation importance for more reliable results\n",
    "    perm_importance = permutation_importance(pipeline, X_test, y_test, n_repeats=10, random_state=42)\n",
    "    perm_importance_mean = perm_importance.importances_mean\n",
    "    \n",
    "    # Create DataFrame for permutation importance\n",
    "    perm_importance_df = pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        'Permutation_Importance': perm_importance_mean\n",
    "    })\n",
    "    perm_importance_df = perm_importance_df.sort_values('Permutation_Importance', ascending=False)\n",
    "    \n",
    "    print(\"\\nTop 15 features by permutation importance:\")\n",
    "    print(perm_importance_df.head(15))\n",
    "\n",
    "# Create DataFrame for model-based importance\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': importance\n",
    "})\n",
    "importance_df = importance_df.sort_values('Importance', ascending=False)\n",
    "\n",
    "print(\"\\nTop 15 features by model importance:\")\n",
    "print(importance_df.head(15))\n",
    "\n",
    "# Calculate percentage contribution to churn\n",
    "total_importance = importance_df['Importance'].sum()\n",
    "importance_df['Contribution_Percentage'] = (importance_df['Importance'] / total_importance) * 100\n",
    "\n",
    "print(\"\\nTop 15 features by contribution percentage:\")\n",
    "print(importance_df[['Feature', 'Contribution_Percentage']].head(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Visualize feature importance\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(x='Contribution_Percentage', y='Feature', data=importance_df.head(15))\n",
    "plt.title(f'Top 15 Features Contributing to Churn ({best_model})')\n",
    "plt.xlabel('Contribution Percentage (%)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. SHAP Analysis for Feature Impact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Try to use SHAP for more detailed analysis\n",
    "try:\n",
    "    # Create a SHAP explainer\n",
    "    explainer = shap.Explainer(model, X_train_processed)\n",
    "    shap_values = explainer(X_train_processed)\n",
    "    \n",
    "    # Plot SHAP summary\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    shap.summary_plot(shap_values, X_train_processed, feature_names=feature_names)\n",
    "    \n",
    "    # Get global feature importance\n",
    "    shap_importance = np.abs(shap_values.values).mean(0)\n",
    "    shap_importance_df = pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        'SHAP_Importance': shap_importance\n",
    "    })\n",
    "    shap_importance_df = shap_importance_df.sort_values('SHAP_Importance', ascending=False)\n",
    "    \n",
    "    print(\"\\nTop 15 features by SHAP importance:\")\n",
    "    print(shap_importance_df.head(15))\n",
    "    \n",
    "    # Calculate percentage contribution based on SHAP\n",
    "    total_shap_importance = shap_importance_df['SHAP_Importance'].sum()\n",
    "    shap_importance_df['SHAP_Contribution_Percentage'] = (shap_importance_df['SHAP_Importance'] / total_shap_importance) * 100\n",
    "    \n",
    "    print(\"\\nTop 15 features by SHAP contribution percentage:\")\n",
    "    print(shap_importance_df[['Feature', 'SHAP_Contribution_Percentage']].head(15))\n",
    "    \n",
    "    # Visualize SHAP contribution percentage\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.barplot(x='SHAP_Contribution_Percentage', y='Feature', data=shap_importance_df.head(15))\n",
    "    plt.title('Top 15 Features Contributing to Churn (SHAP Analysis)')\n",
    "    plt.xlabel('Contribution Percentage (%)')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"SHAP analysis failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Survival Analysis with Cox Proportional Hazards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Prepare data for survival analysis\n",
    "# We need duration (time to event) and event indicator\n",
    "if 'days_rem_to_end_contract' in df_processed.columns:\n",
    "    # Use remaining days to end of contract as duration\n",
    "    df_processed['duration'] = df_processed['days_rem_to_end_contract']\n",
    "elif 'endDate' in df_processed.columns and 'startDate' in df_processed.columns:\n",
    "    # Calculate duration from start to end date\n",
    "    df_processed['startDate'] = pd.to_datetime(df_processed['startDate'])\n",
    "    df_processed['endDate'] = pd.to_datetime(df_processed['endDate'])\n",
    "    df_processed['duration'] = (df_processed['endDate'] - df_processed['startDate']).dt.days\n",
    "else:\n",
    "    print(\"No suitable duration column found for survival analysis\")\n",
    "\n",
    "# Event indicator is lost_program\n",
    "df_processed['event'] = df_processed['lost_program']\n",
    "\n",
    "# Select features for Cox model\n",
    "# Remove highly correlated features and non-numeric columns\n",
    "numeric_cols = df_processed.select_dtypes(include=['int64', 'float64']).columns\n",
    "cox_features = [col for col in numeric_cols if col not in ['lost_program', 'duration', 'event']]\n",
    "\n",
    "# Handle missing values\n",
    "cox_df = df_processed[cox_features + ['duration', 'event']]\n",
    "cox_df = cox_df.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "# Impute missing values\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "cox_df[cox_features] = imputer.fit_transform(cox_df[cox_features])\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "cox_df[cox_features] = scaler.fit_transform(cox_df[cox_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Fit Cox model\n",
    "cph = CoxPHFitter()\n",
    "try:\n",
    "    cph.fit(cox_df, duration_col='duration', event_col='event')\n",
    "    \n",
    "    # Get summary\n",
    "    cox_summary = cph.summary\n",
    "    \n",
    "    # Sort by p-value to find significant features\n",
    "    cox_summary = cox_summary.sort_values('p')\n",
    "    \n",
    "    print(\"\\nCox Proportional Hazards Model Summary (top 15 significant features):\")\n",
    "    print(cox_summary.head(15))\n",
    "    \n",
    "    # Calculate hazard ratio and percentage impact\n",
    "    cox_summary['hazard_ratio'] = np.exp(cox_summary['coef'])\n",
    "    cox_summary['percentage_impact'] = (cox_summary['hazard_ratio'] - 1) * 100\n",
    "    \n",
    "    print(\"\\nFeature impact on churn risk (top 15):\")\n",
    "    impact_summary = cox_summary[['hazard_ratio', 'percentage_impact', 'p']].sort_values('percentage_impact', ascending=False)\n",
    "    print(impact_summary.head(15))\n",
    "    \n",
    "    # Visualize hazard ratios\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    significant_features = cox_summary[cox_summary['p'] < 0.05].sort_values('percentage_impact', ascending=False).head(15)\n",
    "    sns.barplot(x='percentage_impact', y=significant_features.index, data=significant_features)\n",
    "    plt.title('Impact of Features on Churn Risk (Cox Proportional Hazards)')\n",
    "    plt.xlabel('Percentage Impact on Churn Risk (%)')\n",
    "    plt.axvline(x=0, color='r', linestyle='--')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Cox model fitting failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Final Conclusions and Recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top Features Contributing to Churn\n",
    "\n",
    "Based on our comprehensive analysis using multiple statistical models, we've identified the key features that contribute to customer churn. These features and their percentage contributions provide valuable insights into why customers are leaving.\n",
    "\n",
    "### Key Insights:\n",
    "\n",
    "1. **Operational Metrics**: Features related to operational efficiency such as load handling, turn times, and throughput ratios significantly impact churn.\n",
    "\n",
    "2. **Revenue Patterns**: Revenue-related features, especially recent revenue trends (3-month, 6-month), are strong indicators of potential churn.\n",
    "\n",
    "3. **Customer Engagement**: Metrics related to customer activity frequency and recency are important predictors of churn.\n",
    "\n",
    "4. **Contract Terms**: Features related to contract duration and time remaining show significant impact on churn probability.\n",
    "\n",
    "### Recommendations:\n",
    "\n",
    "1. **Proactive Monitoring**: Implement real-time monitoring of the top churn indicators identified in this analysis.\n",
    "\n",
    "2. **Targeted Interventions**: Develop specific intervention strategies for customers showing warning signs in the key metrics.\n",
    "\n",
    "3. **Operational Improvements**: Address the operational issues identified as major contributors to churn.\n",
    "\n",
    "4. **Customer Success Program**: Establish a customer success program focused on improving the metrics most strongly associated with retention.\n",
    "\n",
    "5. **Regular Analysis**: Conduct regular analysis of these metrics to track improvements and identify new patterns."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

